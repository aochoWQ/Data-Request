#Purpose is to download the data for the specific sites sampled for the San Juan Watershed Analysis project

#There were 35 sites identified but only thirteen sites had samples collected in some reports
#Reference documents in https://mail.google.com/mail/u/0/#search/san+juan/FMfcgzGwJvgbFplNtRXCfWrWggXzFtDr?projector=1&messagePartId=0.2
#Looking at all sites on project=Animas_SanJuan_Watershed
```{r}
library(data.table)
library(readxl)
library(dplyr)
library(leaflet)
sites_url="https://www.waterqualitydata.us/data/Station/search?project=Animas_SanJuan_Watershed&startDateLo=01-01-2010&startDateHi=12-31-2023&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"

sj_sites = as.data.frame(data.table::fread(sites_url))
sj_sites = sj_sites%>%
  select(where(~ !all(is.na(.))))
#There are 88 unique sites in the project=Animas_SanJuan_Watershed
write.csv(sj_sites,"sj_sites_2010-2023.csv")# using this file to compare against site list from site list:https://docs.google.com/spreadsheets/d/10swoywTnpwNAMfF2ZBLGOMdji8pJoZHU9ffZvN-6cuo/edit#gid=0
 sj_sites = read.csv("sj_sites_2010-2023.csv")

```

#Sites list for WQP Call
#This file is from https://docs.google.com/spreadsheets/d/10swoywTnpwNAMfF2ZBLGOMdji8pJoZHU9ffZvN-6cuo/edit#gid=0 after translating the names (there were two mistakes in the main names) ALSO Added the note for sites that were in 2023 SAP
```{r}
sj_project_sites=read_excel("SJ_translation_tables.xlsx", sheet = "site_translation") 
sj_project_sites1=sj_project_sites%>%filter(!SJ_PROJECT_SITE=="No")#Filter to only sites within the SJ Watershed Group Project

siteids=paste("siteid=",sj_project_sites1$MonitoringLocationIdentifier,sep="",collapse = "&")
```

#Download data for sites specified above
#Pattern follows: siteid=MLID&
can you download data from the project name of Animas_SanJuan_Watershed? and is this the same amount of data as downloading by site?
```{r}
start_date="startDateLo=01-01-2015"
end_date="startDateHi=12-31-2023"
#Results URL
base0="https://www.waterqualitydata.us/data/Result/search?countrycode=US&project=Animas_SanJuan_Watershed&mimeType=csv&zip=no&dataProfile=narrowResult&providers=NWIS&providers=STEWARDS&providers=STORET"


full_url=paste(base0,start_date,end_date,siteids,sep="&")
full_url=gsub(" ", "%20", full_url)

#****** THESE ARE ONLY SITES IN SITE LIST
results_SJ=as.data.frame(data.table::fread(full_url))#for sites in sitesids list
length(unique(results_SJ$MonitoringLocationIdentifier))
results_SJ=merge(results_SJ,sj_project_sites1)
#39 sites within this
write.csv(results_SJ,"results_SJ7-23-24.csv")

results_SJ = read.csv("results_SJ7-23-24.csv")
```

#******for ALL sites in Project Animas_SanJuan_Watershed - In creating the CharacteristicName translation table and data prep, will it actually be useful to do that now to find all possible scenarios and values? Filter for only SJ Watershed Project sites later? YESYES
```{r}
results_Animas_SJ=as.data.frame(data.table::fread(paste(base0,start_date,end_date,sep="&")))
length(unique(results_Animas_SJ$MonitoringLocationIdentifier))

#merge with sites tables
results_Animas_SJ1=merge(results_Animas_SJ,sj_project_sites)

#download DetectionQuanitationLimits

detquant_path = "https://www.waterqualitydata.us/data/ResultDetectionQuantitationLimit/search?&project=Animas_SanJuan_Watershed&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"
detquant_path=paste(detquant_path,start_date,end_date,sep="&")
sj_detquant=as.data.frame(data.table::fread(detquant_path))
sj_detquant1=sj_detquant%>%
  select(where(~ !all(is.na(.))))

# Instead of the code below where we only filter for LQL and MDL, lets apply the detLimitTypeTable to get the min rank and see if any besides those are the only option. Are there any params or situations where MDL is the the top rank?
det_lim_tab=read_excel("SJ_translation_tables.xlsx", sheet = "detLimitTypeTable") 
det_lim_tab=det_lim_tab%>%select(DetectionQuantitationLimitTypeName,IRLimitPriorityRanking_lower)%>%filter(!is.na(IRLimitPriorityRanking_lower))

sj_detquant1=merge(sj_detquant1,det_lim_tab,all.x=T)

sj_detquant1=sj_detquant1%>%
  filter(!is.na(IRLimitPriorityRanking_lower))%>%
  group_by(ResultIdentifier)%>%
  filter(IRLimitPriorityRanking_lower==min(IRLimitPriorityRanking_lower))%>%
  ungroup()%>%
  select(-IRLimitPriorityRanking_lower,-ProviderName)%>%
  rename(dql_unit=`DetectionQuantitationLimitMeasure/MeasureUnitCode`)%>%
  mutate(dql_unit=tolower(dql_unit))

table(sj_detquant1$DetectionQuantitationLimitTypeName)
table(sj_detquant1$dql_unit)

#*******Change to updated criteria file.
combined_all_criteria2 <- read.csv("SJ_combined_criteria.csv")
#Present Below Quanitifcation Limit is only for characteristicName =="RBP2, Substrate, inorganic, gravel, 2-64 mm"
#Present Above Quantification Limit only for CharacteristicName=="Turbidity"
table(results_Animas_SJ1[results_Animas_SJ1$CharacteristicName%in%combined_all_criteria2$Parameter,]$ResultDetectionConditionText)


 #May need to first check results_Animas_SJ1 in order to check if the metals of interest have any other values other than Non detects
#Convert detquant limits to mg/l before merging?
unit_trans=read_excel("SJ_translation_tables.xlsx", sheet = "UnitConversion") 
unit_trans_dql=unit_trans%>%rename(dql_unit=ResultMeasure_MeasureUnitCode)
#This filtering of only accepted units will filter out some CharacteristicName of Ammonia/Inorganic Nitrogen, E. coli, mg/g or similar weight concentrations like ug/g for metals in sediment.
sj_detquant1=merge(sj_detquant1,unit_trans_dql,all.x=T)%>%
  filter(UnitFlag!="Reject")%>%
  mutate(`DetectionQuantitationLimitMeasure/MeasureValue`=`DetectionQuantitationLimitMeasure/MeasureValue`*UnitConversion,
         dql_unit=CriterionUnit)%>%
  select(-CriterionUnit,-UnitConversion)

table(results_Animas_SJ1$`ResultMeasure/MeasureUnitCode`)

#****** I added the UnitFlag in the sj_detquant1 portion. There are some rows that have no detquant values. 1) Does it matter that they don't have those values? 2) Would be better to add those values to the overall results.

results_Animas_SJ1$`ResultMeasure/MeasureUnitCode`=tolower(results_Animas_SJ1$`ResultMeasure/MeasureUnitCode`) # minimize number of unit translations/combos
table(results_Animas_SJ1$`ResultMeasure/MeasureUnitCode`)


results_Animas_SJ2=merge(results_Animas_SJ1,sj_detquant1,all.x = T) #This is all sites in the basin.. May be interesting to map this vs the results_SJ_only to see if there are other sites that are important or make large impacts.

miss_site=results_Animas_SJ2[results_Animas_SJ2$MonitoringLocationIdentifier=="21NMEX_WQX-67SanJua099.5",]
table(miss_site$SJ_PROJECT_SITE)# IS THIS SITE REALLY NOT IN THE LIST OF SITES? It is not listed as one of the 38 sites.. Can we include all these sites and default filter them out on the Shiny App and only add them if user checks box?

params_interest=c(unique(combined_all_criteria2$Parameter),"Total hardness") #For calculated criteria



results_SJ_metals_only=results_Animas_SJ2[results_Animas_SJ2$CharacteristicName%in%params_interest,]%>%
  select(where(~ !all(is.na(.))))%>%
  mutate(`ResultMeasure/MeasureUnitCode`=ifelse(`ResultMeasure/MeasureUnitCode`=="",dql_unit,`ResultMeasure/MeasureUnitCode`),) #This is to deal with some of the cases where "Present Below Quantification Limit" occurs and the ResultMeasure Unit is an empty string.

results_SJ_metals_only <- results_SJ_metals_only%>%
  mutate(`ResultMeasure/MeasureUnitCode` = ifelse(`ResultMeasure/MeasureUnitCode`=="mg/l caco3**","mg/l" , `ResultMeasure/MeasureUnitCode`)
         ,UnitFlag = ifelse((is.na(UnitFlag)&`ResultMeasure/MeasureUnitCode`%in%c("mg/l" , "ng/l"  ,"ug/l" )),"Accept",UnitFlag))

missing_unit_flag <-  results_SJ_metals_only[is.na(results_SJ_metals_only$UnitFlag),]

table(missing_unit_flag$CharacteristicName)
table(missing_unit_flag$`ResultMeasure/MeasureUnitCode`)

table(results_SJ_metals_only$CharacteristicName,results_SJ_metals_only$ResultDetectionConditionText)
table(results_SJ_metals_only$`ResultMeasure/MeasureUnitCode`)

```
#Nondetects
```{r}
##Investigated counts of non detect values
table(results_SJ_metals_only$ResultDetectionConditionText)
detection_counts=results_SJ_metals_only%>%filter(CharacteristicName%in%combined_all_criteria2$Parameter)%>%
  group_by(CharacteristicName)%>%
  mutate(total=n(),
         nondetect=sum(ResultDetectionConditionText=="Not Detected"),
         percent=nondetect/total*100)%>%
  select(CharacteristicName,total,nondetect,percent)%>%
  distinct()
rm(detection_counts)
```
#Convert all values to mg/l and minimize the columns
```{r}
if(any(!unique(results_SJ_metals_only$`ResultMeasure/MeasureUnitCode`)%in%unit_trans$ResultMeasure_MeasureUnitCode))
{warning("There is a unit not included in the UnitConversion table. Please update") 
missing_units <- unique(results_SJ_metals_only[!results_SJ_metals_only$`ResultMeasure/MeasureUnitCode` %in% unit_trans$ResultMeasure_MeasureUnitCode,]$`ResultMeasure/MeasureUnitCode`)
    print("Missing units:")
    print(missing_units)
} #****** or Shiny Dashboard is it possible to create an input for UnitConversions within shiny?

#write.csv(results_SJ_metals_only,"Results_SJ_metals_only_OG.csv")
#results_SJ_metals_only= read.csv("Results_SJ_metals_only_OG.csv")

results_SJ_metals_only1= results_SJ_metals_only%>%
  rename(ResultMeasure_MeasureUnitCode=`ResultMeasure/MeasureUnitCode`)%>%
  merge(.,unit_trans,all.x=T)%>%
  mutate(SJ_ResultValue=as.numeric(ResultMeasureValue)*UnitConversion,
         SJ_Unit=CriterionUnit)%>%
  select(-`AnalysisStartTime/TimeZoneCode`,-BiologicalIntentName,-SubjectTaxonomicName,-SampleTissueAnatomyName,-ResultTimeBasisText,-ResultWeightBasisText,-StatisticalBaseCode,-DataLoggerLine,-`ActivityStartTime/TimeZoneCode`,-OrganizationIdentifier,-ProviderName,-ResultStatusIdentifier,-`ResultAnalyticalMethod/MethodQualifierTypeName`,-SourceMapScaleNumeric,-MonitoringLocationTypeName,-HUCEightDigitCode,-HorizontalCollectionMethodName,-HorizontalCoordinateReferenceSystemDatumName,-`VerticalMeasure/MeasureValue`,-CountryCode,-CountyCode,-StateCode)

```

#Temporarily adding .5 MDL to Not Detected Samples and flag to add if wanted.
# And missing Total Hardness. results_SJ_metals_only1 has 346 hardness vals vs. 1303 hardness vales from results_SJ_metals_only
```{r}
results_SJ_metals_only1=results_SJ_metals_only1%>%
  mutate(SJ_ND=ifelse(is.na(SJ_ResultValue),"Yes",NA),
         SJ_ResultValue=ifelse(is.na(SJ_ResultValue),`DetectionQuantitationLimitMeasure/MeasureValue`*.5,SJ_ResultValue))


#Column UnitFlag is NA. So need to check when UnitFlag is assigned and why not assigned to Total Hardness values and possibly others..
# Missing hardness samples
missing_hardness=results_SJ_metals_only[results_SJ_metals_only$CharacteristicName=="Total hardness",]
missing_hard1 <- missing_hardness%>%
  filter(!ResultIdentifier%in%results_SJ_metals_only1$ResultIdentifier)

```


# Add Jurisdiction Info Creating DF - results_SJ_metals_only2
```{r}
library(dplyr)
library(sf)
# Filter for only jurisdictions in : combined_all_criteria,"SJ_combined_criteria.csv")
jurisdictions=c("Colorado",  "Navajo Nation",  "New Mexico", "Utah", "Ute Mountain","Southern Ute")

statesJSON <- st_read("./Spatial Layers/states.geojson") #selected_states
sj_states=statesJSON[statesJSON$NAME%in%jurisdictions,]%>%select(NAME,geometry)

tribesJSON <- st_read("./Spatial Layers/tribes.geojson")
sj_tribes=tribesJSON[tribesJSON$NAME%in%jurisdictions,]%>%select(NAME,geometry)

# Looks like Navajo Nation Poly is invalid, keep having issues with it..
# Check for invalid geometries again
invalid_indices <- which(!st_is_valid(sj_tribes))
sj_tribes_ok <- sj_tribes[-invalid_indices, ]
sj_tribes_invalid <- sj_tribes[invalid_indices, ]
# Decompose polygons into simpler parts
decomposed <- st_cast(sj_tribes_invalid, "POLYGON")
# Recompose the polygons
sj_tribes_invalid_fix <- st_union(decomposed)
# Ensure it is a MULTIPOLYGON, even if it's only one feature
if (!inherits(sj_tribes_invalid_fix, "MULTIPOLYGON")) {
  sj_tribes_invalid_fix <- st_cast(sj_tribes_invalid_fix, "MULTIPOLYGON")
}
# Create a new sf data frame
sj_nav_nat <- st_sf(
  NAME = "Navajo Nation",
  geometry = st_sfc(sj_tribes_invalid_fix, crs = st_crs(sj_tribes))
)
# Combine with valid geometries
sj_tribes_final <- rbind(sj_tribes_ok, sj_nav_nat)

sj_tribes_final <- st_transform(sj_tribes_final,crs=4326)
sj_states <- st_transform(sj_states,crs=4326)

tribe_state_poly=rbind(sj_states,sj_tribes_final)%>%
   st_transform(.,crs=4326)#sf polygon shape object

# Create simple list of the sj sites
unique_sj_sites=sj_sites%>%select( MonitoringLocationIdentifier,MonitoringLocationName,LatitudeMeasure,LongitudeMeasure)%>%
  st_as_sf(coords = c("LongitudeMeasure", "LatitudeMeasure"), crs = 4326) %>% 
  st_set_geometry("geometry")

sj_sites_jurisdictions <- st_join(unique_sj_sites, tribe_state_poly, join = st_within)
sj_sites_jurisdictions=sj_sites_jurisdictions%>%
  group_by(MonitoringLocationIdentifier)%>%
  mutate(Jurisdictions=paste(NAME,collapse = ", "))%>%
  select(-NAME)%>%
  ungroup()%>%
  distinct()


#Merge Jurisdiction and further trim column # QWH
results_SJ_metals_only2=merge(results_SJ_metals_only1,sj_sites_jurisdictions)%>%
  select(-MethodSpecificationName,-MeasureQualifierCode,ResultValueTypeName,-ResultCommentText,-`ResultAnalyticalMethod/MethodIdentifier`,-`ResultAnalyticalMethod/MethodIdentifierContext`,-`ResultAnalyticalMethod/MethodName`,-MethodDescriptionText,-ResultDetectionQuantitationLimitUrl,-LaboratoryName,-AnalysisStartDate,-BiologicalIndividualIdentifier)%>%
  mutate(SJ_ND=ifelse(is.na(SJ_ND),"No","Yes")) 

# Are any of the non detect or below method detection limit values above the criteria?

```

 #Plotting exploration..
```{r}
#%>% # ADD A COLUMN FOR PRIMARY JURIS?,Prime_Jurisdiction=ifelse(Jurisdiction)
#  filter(Jurisdictions!="NA")

# print(length(unique(results_SJ_metals_only2$ResultIdentifier)))
# 
# p_names=unique(results_SJ_metals_only2$CharacteristicName)
# print(p_names[1:4])
# ##****** PLOTS
# sum_params = results_SJ_metals_only2%>%
#   filter(ResultSampleFractionText=="Total",SJ_ND=="No",
#          CharacteristicName%in%c("Aluminum","Antimony","Arsenic","Barium"))%>%
#   select(CharacteristicName,SJ_ResultValue,ActivityStartDate)%>%
#   distinct()
# 
# plot <- ggplot(sum_params, aes(x = ActivityStartDate, y = SJ_ResultValue)) +
#   geom_point(size = 3, alpha = 0.6) +  # Adjust point size and transparency
#   labs(title = "Time Series of All Parameters",
#        x = "Sample Dates",
#        y = "mg/L") +
#   facet_wrap(~CharacteristicName, scales = "free_y",ncol = 1) +  # Facet by 'CharacteristicName'
#   theme_minimal() +  # Use a minimal theme
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
# 
# # Print the plot
# print(plot)

# # Create the plot with facet_grid
# plot <- ggplot(sum_params, aes(x = ActivityStartDate, y = SJ_ResultValue)) +
#   geom_point(size = 3, alpha = 0.6) +
#   labs(title = "Time Series of All Parameters",
#        x = "Sample Dates",
#        y = "mg/L") +
#   facet_grid(rows = vars(CharacteristicName), scales = "free_y") +  # Organize by rows
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 
# # Print or save as before
# print(plot)

# gg <- ggplot(sum_params, aes(x = ActivityStartDate, y = SJ_ResultValue)) +
#   geom_point(size = 3, alpha = 0.6) +
#   labs(title = "Time Series of All Parameters",
#        x = "Sample Dates",
#        y = "mg/L") +
#   facet_wrap(~CharacteristicName, scales = "free_y") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 
# ggplotly(gg)  # Convert to an interactive plotly graph
```

#Clean up results_SJ_metals_only2 -- Assigning and calculating criteria within the app may be heavy. Minimize unuses columns to save memory. & Simplify Jurisdictions 
```{r}

 tr_questions  = results_SJ_metals_only2%>%filter(MonitoringLocationIdentifier=="USEPA_REGION8-9423A",UnitFlag == "Accept")

results_SJ_metals_only2 <-results_SJ_metals_only2%>%
   select(-ResultValueTypeName,-`AnalysisStartTime/Time`,-MonitoringLocationDescriptionText,-`VerticalMeasure/MeasureUnitCode`,-VerticalCollectionMethodName,-VerticalCoordinateReferenceSystemDatumName,-dql_unit,-DetectionQuantitationLimitTypeName,-`DetectionQuantitationLimitMeasure/MeasureValue`)%>%
  mutate(Jurisdictions = case_when(Jurisdictions == "New Mexico, Navajo Nation" ~ "Navajo Nation",
                                   Jurisdictions =="Colorado, Ute Mountain" ~ "Ute Mountain",
                                   Jurisdictions == "Utah, Navajo Nation" ~ "Navajo Nation",
                                   Jurisdictions == "Colorado, Southern Ute"~"Southern Ute",
                                   TRUE~Jurisdictions))%>%
  rename(Jurisdiction = Jurisdictions)


results_SJ_metals_only3 = results_SJ_metals_only2
```
# 
#Add the hardness values to parameters with calculated criteria -- It may make sense to aggDV for Hardness so that there is only one value per site per to not duplicate THIS MAY BE IMPORTANT FOR AQUATIC WILDLIFE CONCENTRATIONS ARE HARDNESS DEPENDENT
```{r}
hardness_values=results_SJ_metals_only3%>%
  filter(CharacteristicName=="Total hardness")%>%
  group_by(MonitoringLocationIdentifier,ActivityIdentifier,ActivityStartDate,ResultSampleFractionText)%>%
  summarise(hardness=mean(SJ_ResultValue),
            coun = n())%>%
  distinct()

hardness_values_T=hardness_values%>%filter(ResultSampleFractionText=="Total")%>%
  mutate(ResultSampleFractionText="Total Recoverable")
hardness_values = rbind(hardness_values,hardness_values_T)

results_SJ_metals_only3 <- merge(results_SJ_metals_only3,hardness_values,all.x=T)

# low_hardness=results_SJ_only%>%
#   filter(CharacteristicName=="Total hardness"&ResultMeasureValue<1)

# low_hardness=low_hardness[,c("OrganizationIdentifier","OrganizationFormalName","MonitoringLocationIdentifier","ProviderName", "ActivityIdentifier","ResultIdentifier","CharacteristicName",                                "MonitoringLocationName", "MonitoringLocationTypeName","LatitudeMeasure","LongitudeMeasure","ActivityStartDate" ,"ActivityStartTime/Time", "ResultSampleFractionText","ResultMeasureValue" ,"ResultMeasure/MeasureUnitCode","MeasureQualifierCode" , "ResultStatusIdentifier" ,                           "ResultValueTypeName"  ,"ResultCommentText" ,"ResultAnalyticalMethod/MethodIdentifier",  "ResultAnalyticalMethod/MethodIdentifierContext" , "ResultAnalyticalMethod/MethodName",
# "MethodDescriptionText","LaboratoryName", "ResultDetectionQuantitationLimitUrl"              
# ,"DetectionQuantitationLimitTypeName","DetectionQuantitationLimitMeasure/MeasureValue"   
# , "DetectionQuantitationLimitMeasure/MeasureUnitCode")]
# 
# write.csv(low_hardness,"Low_hardness_samples.csv")
# low_hardness <- read.csv("Low_hardness_samples.csv")

```


#Explore significant differences between Parameters and Jurisdictions
```{r}
#******TODO: Add Criteria to columns and color code red if mean or median exceed criteria.
#*
# Calculate summary statistics for each Parameter within each Jurisdiction
jur_param_summary <- results_SJ_metals_only2 %>%
  group_by(Jurisdictions, CharacteristicName) %>%
  summarise(
    Mean = mean(SJ_ResultValue, na.rm = TRUE),
    Median = median(SJ_ResultValue, na.rm = TRUE),
    SD = sd(SJ_ResultValue, na.rm = TRUE),
    .groups = 'drop'
  )


library(dplyr)

#Kruskal-Wallis test--
results_kw <- results_SJ_metals_only2 %>%
  group_by(Jurisdictions, CharacteristicName,ResultSampleFractionText) %>%
  summarise(
    KruskalTest = if (n_distinct(Jurisdictions) > 1) {
      list(kruskal.test(SJ_ResultValue ~ Jurisdictions, data = cur_data()))
    } else {
      list(NA)  # Return NA or similar to handle groups with insufficient diversity
    },
    .groups = 'drop'
  )


# Extracting p-values from Kruskal-Wallis test results_kw
results_kw$P_Value <- sapply(results_kw$KruskalTest, function(x) x$p.value)

# Filter parameters with significant differences
significant_parameters <- results_kw %>%
  filter(!is.na(P_Value), P_Value < 0.05) %>%
  pull(Parameter)

```
#Explore the type of data included in the results
# Number of samples per day... Should the results be aggregated to the max daily value of each CharacteristicName?
```{r}
table(results_SJ_metals_only2$CharacteristicName)

# sj_characteristics=results_SJ_metals_only2%>%select(CharacteristicName,ResultSampleFractionText)%>%distinct()
# write.csv(sj_characteristics,"sj_params_fractions.csv")

#Over 400 scenarios where more than 2 samples of same parameter sampled on same day. Do we aggregate by daily value? No.. yes? Keep aggDV() function easily accessible
metals_date_summarries=results_SJ_metals_only2%>%
  group_by(MonitoringLocationIdentifier,ActivityStartDate,CharacteristicName,ResultSampleFractionText)%>%
  distinct()%>%
  summarise(act_count=n_distinct(ResultMeasureValue))

```


#Reading Geojson files for all HUC12s - narrowing down list if we want to use a 
```{r}

# Create a bounding box from sj_sites
bbox <- st_bbox(st_sf(sj_sites_jurisdictions))
# Convert the bbox to an sf polygon
bbox_poly <- st_as_sfc(bbox)


# Set the path to the File Geodatabase
gdb_path <- "/Users/alanochoa/Documents/GitHub/Data_Request/San Juan/wbdhu12_a_us_september2023.gdb"
layers <- st_layers(gdb_path)
# List all layers in the File Geodatabase
# If layers are found, read each layer into a list of sf objects
list_of_layers <- lapply(layers$name, function(layer_name) {
  st_read(gdb_path, layer = layer_name)
})

# Check which HUC12 polygons intersect with the bounding box
states_list=c("UT","NM","CO","AZ","AZ,NM","AZ,UT","AZ,CO,NM,UT","AZ,CO,UT","CO,NM","CO,UT")
huc12_list=list_of_layers[[1]][list_of_layers[[1]]$states%in%states_list,]
huc12_list=st_transform(huc12_list,st_crs(4326))
huc12_list <- huc12_list[st_intersects(huc12_list, bbox_poly, sparse = FALSE), ]



# Performing a spatial join
sj_sits_hucs <- st_join( sites_sf,huc12_list, join = st_within)
sj_hucs=huc12_list[huc12_list$huc12%in%sj_sits_hucs$huc12,]
# This will include all rows from huc12_list where there's at least one matching sj_sites coordinate
other_hucs=c("140801041005","140801050109","140801050105")


rm(list_of_layers,gdb_path,states_list,layers,bbox,bbox_poly)
```

#LOCAL TEST -- Percentiles Test
```{r}
 
library(dplyr)
start=as.Date("2015-01-01")
end=as.Date("2023-10-10")

library(sf)
filteredData1<- results_SJ_metals_only3 %>%
      filter(ResultSampleFractionText %in% c("Total"),
              SJ_ND != "Yes",
              SJ_PROJECT_SITE!="No",
             CharacteristicName %in% c("Arsenic"))%>%
  filter(ActivityStartDate>start &ActivityStartDate<end)

filteredData2 = filteredData1 %>%
      group_by(CharacteristicName, MonitoringLocationIdentifier, ResultSampleFractionText) %>%
      summarise(
        AverageValue = mean(SJ_ResultValue, na.rm = TRUE),  
        Latitude = first(LatitudeMeasure),  # 
        Longitude = first(LongitudeMeasure),  # 
        .groups = "drop"  # This drops the grouping so you can use the data frame normally afterwards
      )%>%
  mutate(Quartile = ntile(AverageValue, 4))

# Define color palette for quartiles
quartile_colors <- c("1" = "green", "2" = "yellow", "3" = "orange", "4" = "red")

library(ggplot2)




# Calculate the quartile boundaries
quartiles <- quantile(filteredData1$AverageValue, probs = c(0.25, 0.5,0.75), na.rm = TRUE)

quartiles2 <- quantile(filteredData2$SJ_ResultValue, probs = c(0.25, 0.5,0.75), na.rm = TRUE)
# > print(quartiles2)
#   25%   50%   75% 
# 0.614 3.600 8.500 
# > print(quartiles)
#   25%   50%   75% 
#  5.45  7.10 21.30 

 print(quartiles)
q25 <- quartiles[1]
q50 <- quartiles[2]
q75 <- quartiles[3]


# Create the plot
plot <- ggplot(filteredData1, aes(x = AverageValue)) +
  annotate("rect", xmin = -Inf, xmax = q25, ymin = -Inf, ymax = Inf, alpha = 0.3, fill = "green") +
  annotate("rect", xmin = q25, xmax = q50, ymin = -Inf, ymax = Inf, alpha = 0.3, fill = "yellow") +
  annotate("rect", xmin = q50, xmax = q75, ymin = -Inf, ymax = Inf, alpha = 0.3, fill = "orange") +
  annotate("rect", xmin = q75, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "red") +
  geom_point(stat='count', aes(y=..count..), size = 5, color = "darkblue", fill = "grey") +
  scale_y_continuous(limits = c(-1, NA), breaks = NULL) + 
  labs(title = "Aluminum Concentration SJ Watershed",
       x = "Average Aluminum Concentration (mg/L)",
       y = NULL,
       caption = "Green: Good, Yellow: Fair, Red: Poor") +
  theme_minimal()+
  theme(
    plot.title = element_text(hjust = 0.5),
    aspect.ratio = 0.2  # Adjust aspect ratio to make it very wide
  )

ggsave(filename = "aluminum_concentration_benchmarks.png", plot = plot, width = 16, height = 4, units = "in", dpi = 300)

ggplot(filteredData1, aes(x = AverageValue)) +
  geom_density(fill = "lightblue", color = "darkblue", alpha = 0.7) +
  labs(title = "Density of Average Aluminum Concentration",
       x = "Average Aluminum Concentration (mg/L)",
       y = "Density") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )

box_plot <- ggplot(filteredData1, aes(x = "", y = AverageValue)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  labs(title = "Distribution of Average Aluminum Concentration",
       x = "",
       y = "Average Aluminum Concentration (mg/L)") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )


filteredData2<- results_SJ_metals_only3 %>%
      filter(ResultSampleFractionText %in% c("Total"),
              SJ_ND != "Yes",
              SJ_PROJECT_SITE!="No",
             CharacteristicName %in% c("Aluminum"))%>%
  filter(ActivityStartDate>start &ActivityStartDate<end)




```


#LOCAL TEST Assigning Criteria and Calcualted Criteria
```{r}
#Filter out the criteria that has been pre-calculated by Kate Sullivan
combined_all_criteria3 <- combined_all_criteria2 %>% rename(ResultSampleFractionText = CriteriaFraction, CharacteristicName = Parameter) 
combined_all_criteria3 <- combined_all_criteria3 %>%
  group_by(Screening.Criteria, Use, Jurisdiction, CharacteristicName, ResultSampleFractionText) %>%
  summarise(
    Criteria_mg_L = ifelse(any(!is.na(Criteria_mg_L)), max(Criteria_mg_L, na.rm = TRUE), NA_real_),
    CF = ifelse(any(!is.na(CF)), max(CF, na.rm = TRUE), NA_character_),
    Criterion_Formula_mgL = ifelse(any(!is.na(Criterion_Formula_mgL)), max(Criterion_Formula_mgL, na.rm = TRUE), NA_character_),
    .groups = 'drop'
  )%>%
  mutate(Criteria_mg_L = ifelse(!is.na(CF),NA,Criteria_mg_L))


# Function to evaluate formulas
evaluate_formula <- function(formula, hardness) {
  if (is.na(formula) || is.na(hardness)) return(NA)
  eval(parse(text = str_replace_all(formula,"hardness",as.character(hardness))))
}

#flag if needs hardness but doesnt have it:
depends_on_hardness <- function(formula) {
  if (is.na(formula)) return(FALSE)
  return(str_detect(formula, "hardness"))
}


##*******-------------There are duplicate samples being counted twice. Why? Criteria that has a defined criteria and calculated formula as well.
##*
#Create subset that will be based off of app filters
subset_sj <- results_SJ_metals_only3 %>%
          filter(ResultSampleFractionText %in% c("Dissolved"),
                 CharacteristicName %in% c("Uranium"),
                 Jurisdiction == "Colorado",
                 UnitFlag == "Accept")


#Testing Subsets
subset_criteria <- combined_all_criteria4 %>%
  filter(ResultSampleFractionText == "Dissolved",
         CharacteristicName == "Uranium",
         Jurisdiction == "Colorado")
        
subset_sj1 <- merge(subset_sj, subset_criteria)%>%
  mutate(flag_missing_hardness = (sapply(CF, depends_on_hardness) | sapply(Criterion_Formula_mgL, depends_on_hardness)) &   is.na(hardness))%>%
  rowwise() %>%
  mutate(CF_calculated = ifelse(sapply(CF, depends_on_hardness) & !is.na(hardness),
                           evaluate_formula(CF, hardness), as.numeric(CF)),
    criteria_calculated = ifelse(!is.na(Criterion_Formula_mgL),
                                 eval(parse(text = str_replace_all(Criterion_Formula_mgL, 
                                                                   c("CF" = as.character(CF_calculated),
                                                                     "hardness" = as.character(hardness))))), Criteria_mg_L),
    exceed = ifelse(SJ_ResultValue > criteria_calculated, 1, 0),
    exceed = ifelse(is.na(exceed), 0, exceed),
    missing_hardness = ifelse(flag_missing_hardness == TRUE, 1, 0)) %>%
  ungroup()

# Use exceedance summary to plot data on time series with criteria once use clicks on site on map.
exceedance_summary <- subset_sj %>%
  group_by(MonitoringLocationIdentifier, LatitudeMeasure, LongitudeMeasure, CharacteristicName, Screening.Criteria, Use, Jurisdiction, ResultSampleFractionText) %>%
  summarize(missing_hardness = sum(missing_hardness),
            sample_count = n() - missing_hardness,
            exceedance = sum(exceed),
            exceed_perc = exceedance / sample_count,
            .groups = 'drop')

avg_exceedance = exceedance_summary%>%
  mutate(exceed_perc=ifelse(is.na(exceed_perc),0,exceed_perc))%>%
  group_by(MonitoringLocationIdentifier)%>%
  summarise(avg_exceedance = mean(exceed_perc))


# Normalize the avg_exceedance values
avg_exceedance1 <- avg_exceedance %>%
  mutate(normalized_exceedance = (avg_exceedance - min(avg_exceedance)) / (max(avg_exceedance) - min(avg_exceedance)))

# Create a color gradient from yellow to red
color_function <- col_numeric(palette = c("yellow", "red"), domain = range(avg_exceedance$avg_exceedance, na.rm = TRUE))
    
    

# Apply the colors to the data
avg_exceedance1 <- avg_exceedance1 %>%
  mutate(color = color_function(avg_exceedance))


na_jurisdiction = results_SJ_metals_only2[results_SJ_metals_only2$Jurisdiction=="NA",]
#table(na_jurisdiction$MonitoringLocationIdentifier)

```
#Site exploration.. There are multiple duplicate sites that have the same coordinates.
```{r}
site_coord <- results_SJ_metals_only3%>%select(MonitoringLocationIdentifier,LatitudeMeasure,LongitudeMeasure)%>%
  distinct()%>%
  mutate(coord_id=paste0(LatitudeMeasure,LongitudeMeasure))


results_SJ_metals_only3$MonitoringLocationIdentifier_OG = results_SJ_metals_only3$MonitoringLocationIdentifier

results_SJ_metals_only3 <- results_SJ_metals_only3 %>%
  mutate(MonitoringLocationIdentifier = case_when(MonitoringLocationIdentifier=="21NMEX_WQX-64SanJua101.6"~"USEPA_REGION8-64SanJua101.6",
          MonitoringLocationIdentifier=="21NMEX_WQX-67SanJua088.1"~"USEPA_REGION8-67SanJua088.1",
          MonitoringLocationIdentifier=="21NMEX_WQX-66Animas002.3"~"USEPA_REGION8-WIIN-13",
          MonitoringLocationIdentifier=="21NMEX_WQX-67SanJua046.0"~"USEPA_REGION8-SJSR",
          MonitoringLocationIdentifier=="21NMEX_WQX-66Animas028.1"~"USEPA_REGION8-66Animas028.1",
          MonitoringLocationIdentifier=="21NMEX_WQX-66Animas044.8"~"USEPA_REGION8-ADW-022",
          MonitoringLocationIdentifier %in% c("21NMEX_WQX-66Animas061.2","SOUTHUTE-AR 1-9")~"USEPA_REGION8-AR 1-9",
          MonitoringLocationIdentifier%in%c("SOUTHUTE-SUIT1","SOUTHUTE-SUIT10",  "SOUTHUTE-SUIT2" ,"SOUTHUTE-SUIT3", "SOUTHUTE-SUIT4", "SOUTHUTE-SUIT5","SOUTHUTE-SUIT6","SOUTHUTE-SUIT7","SOUTHUTE-SUIT8","SOUTHUTE-SUIT9" )~"SOUTHUTE-SUIT1",
          TRUE ~ MonitoringLocationIdentifier))
```

#Append Southern Ute Criteria
# Downloading to manually fix the criteria listed as Total where they are actually Total Recoverable. Must remove the duplicates and then change the formula ones for 
```{r}
criteria3_check <-  combined_all_criteria3%>% group_by(Screening.Criteria,Use,                     Jurisdiction,CharacteristicName)%>%summarize(criteria_count = n())
# Need to remove where Criteria_mg_L==8.838263 (Colorado, AquaticAcute, Aluminum) & Criteria_mg_L==15.400113 (New Mexico, AquaticAcute, Aluminum)
tr_results_SJ_metals_only2=results_SJ_metals_only2%>%filter(ResultSampleFractionText=="Total Recoverable")

write.csv(combined_all_criteria3,"combined_all_criteria3.csv",na="")# Downloading to manually fix the criteria listed as Total where they are actually Total Recoverable. Must remove the duplicates and then change the formula ones for 
combined_all_criteria4 = read.csv("combined_all_criteria4.csv",na.strings = c("","NA","NULL"))
combined_all_criteria4 <- combined_all_criteria4%>%
  mutate(CF = ifelse(CF=="",NA,CF),
        Criterion_Formula_mgL= ifelse(Criterion_Formula_mgL=="",NA,Criterion_Formula_mgL),
        ResultSampleFractionText = ifelse(ResultSampleFractionText=="Total recoverable","Total Recoverable",ResultSampleFractionText))%>%
  select(-Comments)

su_crit <- read.csv("/Users/alanochoa/Documents/GitHub/Data_Request/San Juan/SU_WQ Standards Tables.xlsx - Combined_criteria_SU.csv",na.strings = c("", "NA", "NULL"))

combined_all_criteria4 = rbind(combined_all_criteria4,su_crit)

write.csv(combined_all_criteria4,"combined_all_criteria_7_29_24.csv")

print(range(combined_all_criteria4$Criteria_mg_L))
```
# Remove the Blank samples
```{r}
nrow(results_SJ_metals_only3)

results_SJ_metals_only3 <- results_SJ_metals_only3%>%
  filter(!grepl("QCBLNK",ActivityIdentifier)) 

nrow(results_SJ_metals_only4)
```


# Create the .RData file to use for the app.R dashboard.
```{r}
save(results_SJ_metals_only3,combined_all_criteria4,sj_tribes_final,combined_all_criteria3,file="SJ_WQ_Dash/SJ_dashboard_data.RData")

load("SJ_WQ_Dash/SJ_dashboard_data.RData")
```





